#https://mp.weixin.qq.com/s/MSWhnpUlG4nYkTUZrD_T2A
# image classification tricks
## 1.Warmup
	问题：由于刚开始训练模型的权重是随机初始化的(全部设置为0是一个坑)，此时选择一个较大的学习率，可能会带来模型的不稳定。
	技巧：
	1.constant warmup。使用学习率预热。在刚开始时训练时候先使用一个较小的学习率，训练一些epochs和iterations后，等模型稳定时候再修改Wie预先设置的学习率训练。

	2.gradual warmup
	解决问题：因为从一个很小的学习率转换为一个较大的学习率可能会导致训练误差增大；
	该方法介绍：从最开始的小学习率开始，每个iteration增大一点，直到达到最初设置的比较大的学习率。

## 2. Linear scaling learning rate
	问题：针对对较大的batch_size。
	在凸优化问题中，batch_size增加，收敛速度会降低，处理相同数据量的速度会越来越快，但是达到相同精度所需要的的epoch数量会越来越多。即，使用相同的epoch, 大batach_size训练的模型比小batch_size训练的模型相比，验证准确率会减小。
	
	解决方法：
	1.gradual warmup
	2.linear scaling learning rate
	在mini-batch SGD训练时，梯度下降的值是随机的，因为每一个batch的数据是随机选择的。增大batch size不会改变梯度的期望，但是会降低它的方差。也就是说，大batch size会降低梯度中的噪声，所以我们可以增大学习率来加快收敛。
	具体做法很简单，比如batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 × b/256。

## 3.Label-smoothing
	解决问题：
one-hot编码和通过降低交叉熵损失来调整参数的而方式结合起来，会让模型过分相信它的判断，但是对于一个由多人标注的数据集，每个人的标注也可能会有一些错误，模型对标签的过分信任会导致过拟合。
	具体思想：降低我们对于标签的信任。
	该方法是一种通过在标签y中加入噪声，实现对模型的约束，降低模型过拟合程度的一种正则化方法。

## 4.random image cropping and patching
	做法：随机裁剪四个图片的中部分，然后把它们拼接为一个图片，同时混合这四个图片的标签。
	效果：PICAR在cifar10上达到了2.19%的错误率。

## 5.Knowledge Distillation(知识蒸馏)
	应对问题：
提高几乎所有机器学习算法性能的一种非常简单的方法是在相同的数据上训练许多不同的模型，然后对它们的预测进行平均。但是使用所有的模型集成进行预测是比较麻烦的，并且可能计算量太大而无法部署到大量用户。

## 6.Cutout(正则化)
	方法：训练时候随机把图片的一部分裁剪掉，这样能提高模型的鲁棒性。
	来源：计算机视觉中经常遇到的物体遮挡问题。
	效果：通过cutout生成一些类似被遮挡的物体，不仅可以让模型在遇到遮挡问题时表现更好，还能让模型在做决定时更多地考虑环境(context)。
##7.Random erasing(正则化)
	遮挡物体的数据增强方法
	思想： 类似于cutout方法。
	差异：
	1.cutout是把随机抽取的矩形区域像素置0，相当于裁减掉；random erasing 使用随机数或者数据集中像素的平均值替换原来的像素置。
	2.cutout每次裁剪的区域大小是固定的，Random erasing替换掉的区域大小是随机的。
## 8.Cosine learning rate decay

## 9.Mixup training
	是一种新的数据增强方法。
	


		

